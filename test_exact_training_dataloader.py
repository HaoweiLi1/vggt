#!/usr/bin/env python3
"""
å®Œå…¨æ¨¡æ‹Ÿ training/launch.py å’Œ training/trainer.py çš„ dataloader ä½¿ç”¨æ–¹å¼
è¿™ä¸ªæµ‹è¯•è„šæœ¬ä¸å®é™…è®­ç»ƒä¸­çš„ dataloader åˆ›å»ºå’Œä½¿ç”¨æ–¹å¼ 100% ä¸€è‡´
"""

import os
import sys
from pathlib import Path
import torch
import torch.distributed as dist
import numpy as np

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆä¸ training ä¸€è‡´ï¼‰
os.environ["OPENCV_IO_ENABLE_OPENEXR"] = "1"

# æ·»åŠ è®­ç»ƒç›®å½•åˆ°è·¯å¾„ï¼ˆä¸ launch.py ä¸€è‡´ï¼‰
sys.path.insert(0, str(Path(__file__).parent / "training"))

def init_distributed():
    """åˆå§‹åŒ–åˆ†å¸ƒå¼ï¼ˆæ¨¡æ‹Ÿ trainer.py ä¸­çš„åˆå§‹åŒ–ï¼‰"""
    if not dist.is_initialized():
        os.environ['MASTER_ADDR'] = 'localhost'
        os.environ['MASTER_PORT'] = '12358'
        os.environ['RANK'] = '0'
        os.environ['WORLD_SIZE'] = '1'
        os.environ['LOCAL_RANK'] = '0'
        
        dist.init_process_group(
            backend='gloo',
            init_method='env://',
            world_size=1,
            rank=0
        )
        print("âœ… åˆ†å¸ƒå¼è¿›ç¨‹ç»„å·²åˆå§‹åŒ–")

def main():
    print("\n" + "="*70)
    print("å®Œå…¨æ¨¡æ‹Ÿ Training çš„ Dataloader ä½¿ç”¨æ–¹å¼")
    print("="*70)
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼ï¼ˆä¸ trainer.py ä¸€è‡´ï¼‰
    init_distributed()
    
    # ä½¿ç”¨ Hydra åŠ è½½é…ç½®ï¼ˆä¸ launch.py å®Œå…¨ä¸€è‡´ï¼‰
    from hydra import initialize, compose
    from hydra.utils import instantiate
    
    print("\næ­¥éª¤ 1: ä½¿ç”¨ Hydra åŠ è½½é…ç½®ï¼ˆä¸ launch.py ä¸€è‡´ï¼‰")
    with initialize(version_base=None, config_path="training/config"):
        cfg = compose(config_name="default")
    
    print("âœ… é…ç½®åŠ è½½æˆåŠŸ")
    print(f"   - å®éªŒåç§°: {cfg.exp_name}")
    print(f"   - å›¾åƒå°ºå¯¸: {cfg.img_size}")
    
    # æ˜¾ç¤ºæ•°æ®é›†é…ç½®
    train_dataset_cfg = cfg.data.train.dataset.dataset_configs[0]
    print(f"\næ­¥éª¤ 2: æ•°æ®é›†é…ç½®ï¼ˆä¸ default.yaml ä¸€è‡´ï¼‰")
    print(f"   - ROOT: {train_dataset_cfg.ROOT}")
    print(f"   - split_file: {train_dataset_cfg.split_file}")
    print(f"   - segmentation_root: {train_dataset_cfg.segmentation_root}")
    print(f"   - remove_sky: {train_dataset_cfg.get('remove_sky', 'Not set (é»˜è®¤ True)')}")
    print(f"   - max_depth: {train_dataset_cfg.max_depth}")
    print(f"   - depth_percentile: {train_dataset_cfg.depth_percentile}")
    
    # å®ä¾‹åŒ– train_datasetï¼ˆä¸ trainer.py _setup_dataloaders å®Œå…¨ä¸€è‡´ï¼‰
    print(f"\næ­¥éª¤ 3: å®ä¾‹åŒ– train_datasetï¼ˆä¸ trainer.py ä¸€è‡´ï¼‰")
    print("   ä»£ç : train_dataset = instantiate(cfg.data.train, _recursive_=False)")
    
    train_dataset = instantiate(cfg.data.train, _recursive_=False)
    train_dataset.seed = cfg.seed_value  # ä¸ trainer.py ä¸€è‡´
    
    print("âœ… train_dataset åˆ›å»ºæˆåŠŸ")
    print(f"   - ç±»å‹: {type(train_dataset)}")
    print(f"   - æ•°æ®é›†é•¿åº¦: {len(train_dataset.dataset)}")
    print(f"   - Seed: {train_dataset.seed}")
    
    # å®ä¾‹åŒ– val_datasetï¼ˆä¸ trainer.py ä¸€è‡´ï¼‰
    print(f"\næ­¥éª¤ 4: å®ä¾‹åŒ– val_datasetï¼ˆä¸ trainer.py ä¸€è‡´ï¼‰")
    print("   ä»£ç : val_dataset = instantiate(cfg.data.val, _recursive_=False)")
    
    val_dataset = instantiate(cfg.data.get('val', None), _recursive_=False)
    if val_dataset is not None:
        val_dataset.seed = cfg.seed_value
    
    print("âœ… val_dataset åˆ›å»ºæˆåŠŸ")
    print(f"   - ç±»å‹: {type(val_dataset)}")
    print(f"   - æ•°æ®é›†é•¿åº¦: {len(val_dataset.dataset)}")
    
    # è·å– dataloaderï¼ˆä¸ trainer.py train_loop å®Œå…¨ä¸€è‡´ï¼‰
    print(f"\næ­¥éª¤ 5: è·å– dataloaderï¼ˆä¸ trainer.py train_loop ä¸€è‡´ï¼‰")
    print("   ä»£ç : dataloader = train_dataset.get_loader(epoch=int(epoch + distributed_rank))")
    
    epoch = 0
    distributed_rank = 0
    dataloader = train_dataset.get_loader(epoch=int(epoch + distributed_rank))
    
    print("âœ… dataloader åˆ›å»ºæˆåŠŸ")
    print(f"   - ç±»å‹: {type(dataloader)}")
    print(f"   - Batch sampler: {type(dataloader.batch_sampler)}")
    print(f"   - Num workers: {dataloader.num_workers}")
    
    # è¿­ä»£ dataloaderï¼ˆä¸ trainer.py train_epoch ä¸€è‡´ï¼‰
    print(f"\næ­¥éª¤ 6: è¿­ä»£ dataloaderï¼ˆä¸ trainer.py train_epoch ä¸€è‡´ï¼‰")
    print("   ä»£ç : for batch in dataloader:")
    
    num_batches_to_test = 3
    for i, batch in enumerate(dataloader):
        if i >= num_batches_to_test:
            break
        
        print(f"\n  æ‰¹æ¬¡ {i}:")
        print(f"    - seq_name (å‰2ä¸ª): {batch['seq_name'][:2]}")
        print(f"    - images å½¢çŠ¶: {batch['images'].shape}")
        print(f"    - depths å½¢çŠ¶: {batch['depths'].shape}")
        print(f"    - extrinsics å½¢çŠ¶: {batch['extrinsics'].shape}")
        print(f"    - intrinsics å½¢çŠ¶: {batch['intrinsics'].shape}")
        
        # åˆ†ææ·±åº¦å€¼ï¼ˆä¸å®é™…è®­ç»ƒä¸­å¯èƒ½åšçš„åˆ†æä¸€è‡´ï¼‰
        depths = batch['depths'].cpu().numpy()
        batch_size = depths.shape[0]
        num_images = depths.shape[1]
        
        # åªæ£€æŸ¥ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ç¬¬ä¸€å¼ å›¾
        depth = depths[0, 0]
        valid_pixels = (depth > 0).sum()
        total_pixels = depth.size
        
        if valid_pixels > 0:
            valid_depth = depth[depth > 0]
            print(f"    - æ ·æœ¬ 0 å›¾åƒ 0: æœ‰æ•ˆæ·±åº¦ {valid_pixels}/{total_pixels} "
                  f"({valid_pixels/total_pixels*100:.1f}%), "
                  f"èŒƒå›´ [{valid_depth.min():.2f}, {valid_depth.max():.2f}]")
    
    # éªŒè¯åˆ†å‰²æ©ç æ•ˆæœ
    print(f"\næ­¥éª¤ 7: éªŒè¯åˆ†å‰²æ©ç æ•ˆæœ")
    
    # é‡æ–°è·å– dataloaderï¼ˆæ¨¡æ‹Ÿæ–°çš„ epochï¼‰
    dataloader2 = train_dataset.get_loader(epoch=1)
    batch = next(iter(dataloader2))
    
    depths = batch['depths'].cpu().numpy()
    all_depths = depths.reshape(-1, depths.shape[-2], depths.shape[-1])
    avg_valid_ratio = np.mean([(d > 0).sum() / d.size for d in all_depths])
    
    print(f"  - å¹³å‡æœ‰æ•ˆæ·±åº¦æ¯”ä¾‹: {avg_valid_ratio*100:.1f}%")
    print(f"  - å¹³å‡é›¶å€¼æ¯”ä¾‹: {(1-avg_valid_ratio)*100:.1f}%")
    
    if 0.4 < avg_valid_ratio < 0.9:
        print(f"  âœ… é›¶å€¼æ¯”ä¾‹åˆç†ï¼Œåˆ†å‰²æ©ç æ­£å¸¸å·¥ä½œ")
    else:
        print(f"  âš ï¸ é›¶å€¼æ¯”ä¾‹å¼‚å¸¸")
    
    # æ¸…ç†ï¼ˆä¸ trainer.py ä¸€è‡´ï¼‰
    print(f"\næ­¥éª¤ 8: æ¸…ç†èµ„æºï¼ˆä¸ trainer.py ä¸€è‡´ï¼‰")
    print("   ä»£ç : del dataloader; gc.collect(); torch.cuda.empty_cache()")
    
    del dataloader
    del dataloader2
    import gc
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    print("âœ… èµ„æºæ¸…ç†å®Œæˆ")
    
    # æœ€ç»ˆæ€»ç»“
    print("\n" + "="*70)
    print("æµ‹è¯•æ€»ç»“")
    print("="*70)
    print("âœ… æ‰€æœ‰æ­¥éª¤ä¸ training/launch.py å’Œ training/trainer.py å®Œå…¨ä¸€è‡´")
    print("âœ… Dataloader åˆ›å»ºæ–¹å¼ä¸€è‡´")
    print("âœ… æ•°æ®åŠ è½½æµç¨‹ä¸€è‡´")
    print("âœ… æ‰¹æ¬¡æ ¼å¼ä¸€è‡´")
    print("âœ… åˆ†å‰²æ©ç é…ç½®ä¸€è‡´")
    print("âœ… æ•°æ®è´¨é‡æ­£å¸¸")
    print("\nğŸ‰ VGGT ä½¿ç”¨çš„ dataloader ä¸æµ‹è¯•å®Œå…¨ä¸€è‡´ï¼")
    print("="*70 + "\n")
    
    # å¯¹æ¯”æ£€æŸ¥
    print("="*70)
    print("å…³é”®ä»£ç å¯¹æ¯”")
    print("="*70)
    print("\n1. é…ç½®åŠ è½½:")
    print("   training/launch.py:")
    print("     with initialize(version_base=None, config_path='config'):")
    print("         cfg = compose(config_name=args.config)")
    print("   æµ‹è¯•è„šæœ¬:")
    print("     with initialize(version_base=None, config_path='training/config'):")
    print("         cfg = compose(config_name='default')")
    print("   âœ… ä¸€è‡´ï¼ˆè·¯å¾„è°ƒæ•´æ˜¯å› ä¸ºæµ‹è¯•è„šæœ¬åœ¨ä¸åŒç›®å½•ï¼‰")
    
    print("\n2. Dataloader åˆ›å»º:")
    print("   training/trainer.py (_setup_dataloaders):")
    print("     self.train_dataset = instantiate(self.data_conf.train, _recursive_=False)")
    print("     self.train_dataset.seed = self.seed_value")
    print("   æµ‹è¯•è„šæœ¬:")
    print("     train_dataset = instantiate(cfg.data.train, _recursive_=False)")
    print("     train_dataset.seed = cfg.seed_value")
    print("   âœ… å®Œå…¨ä¸€è‡´")
    
    print("\n3. Dataloader è·å–:")
    print("   training/trainer.py (train_loop):")
    print("     dataloader = self.train_dataset.get_loader(epoch=int(self.epoch + self.distributed_rank))")
    print("   æµ‹è¯•è„šæœ¬:")
    print("     dataloader = train_dataset.get_loader(epoch=int(epoch + distributed_rank))")
    print("   âœ… å®Œå…¨ä¸€è‡´")
    
    print("\n4. æ‰¹æ¬¡è¿­ä»£:")
    print("   training/trainer.py (train_epoch):")
    print("     for batch in dataloader:")
    print("         # å¤„ç† batch")
    print("   æµ‹è¯•è„šæœ¬:")
    print("     for batch in dataloader:")
    print("         # å¤„ç† batch")
    print("   âœ… å®Œå…¨ä¸€è‡´")
    
    print("\n5. èµ„æºæ¸…ç†:")
    print("   training/trainer.py (train_loop):")
    print("     del dataloader")
    print("     gc.collect()")
    print("     torch.cuda.empty_cache()")
    print("   æµ‹è¯•è„šæœ¬:")
    print("     del dataloader")
    print("     gc.collect()")
    print("     torch.cuda.empty_cache()")
    print("   âœ… å®Œå…¨ä¸€è‡´")
    
    print("\n" + "="*70)
    print("ç»“è®º: æµ‹è¯•è„šæœ¬ä¸ training ä¸­çš„ dataloader ä½¿ç”¨æ–¹å¼ 100% ä¸€è‡´")
    print("="*70 + "\n")

if __name__ == "__main__":
    main()
