# æ˜¾å­˜ä¼˜åŒ–æ€»ç»“

## ğŸ¯ é—®é¢˜
è¿è¡Œ `compare_models.py` æ—¶é‡åˆ° CUDA OOM (Out of Memory) é”™è¯¯ï¼š
```
âŒ Failed to load baseline model: CUDA out of memory. Tried to allocate 16.00 MiB.
```

## ğŸ” åŸå› åˆ†æ

### åŸå§‹å®ç°çš„é—®é¢˜
```python
# âŒ é”™è¯¯åšæ³•ï¼šåŒæ—¶åŠ è½½ä¸¤ä¸ªæ¨¡å‹åˆ°æ˜¾å­˜
finetuned_model = load_model_from_pt(args.finetuned_model, device)
baseline_model = load_model_from_pt(args.baseline_model, device)

for img_file in image_files:
    finetuned_depth = predict_depth(finetuned_model, img_file, device, dtype)
    baseline_depth = predict_depth(baseline_model, img_file, device, dtype)
    create_comparison(...)
```

**é—®é¢˜**:
- VGGT æ¨¡å‹å¾ˆå¤§ï¼ˆ~5GBï¼‰
- åŒæ—¶åŠ è½½ä¸¤ä¸ªæ¨¡å‹éœ€è¦ ~10GB æ˜¾å­˜
- åŠ ä¸Šæ¨ç†æ—¶çš„ä¸­é—´æ¿€æ´»å€¼ï¼Œæ€»æ˜¾å­˜éœ€æ±‚ > 12GB
- å¤§å¤šæ•°æ¶ˆè´¹çº§ GPU æ— æ³•æ»¡è¶³

## âœ… è§£å†³æ–¹æ¡ˆ

### åˆ†é˜¶æ®µå¤„ç†ç­–ç•¥
```python
# âœ… æ­£ç¡®åšæ³•ï¼šåˆ†é˜¶æ®µåŠ è½½æ¨¡å‹

# é˜¶æ®µ 1: Fine-tuned æ¨¡å‹é¢„æµ‹
finetuned_model = load_model_from_pt(args.finetuned_model, device)
for data in image_data:
    data['finetuned_depth'] = predict_depth(finetuned_model, ...)
del finetuned_model
torch.cuda.empty_cache()  # é‡Šæ”¾æ˜¾å­˜

# é˜¶æ®µ 2: Baseline æ¨¡å‹é¢„æµ‹
baseline_model = load_model_from_pt(args.baseline_model, device)
for data in image_data:
    data['baseline_depth'] = predict_depth(baseline_model, ...)
del baseline_model
torch.cuda.empty_cache()  # é‡Šæ”¾æ˜¾å­˜

# é˜¶æ®µ 3: ç”Ÿæˆå¯¹æ¯”å›¾ï¼ˆä¸éœ€è¦æ¨¡å‹ï¼‰
for data in image_data:
    create_comparison_figure(...)
```

## ğŸ“Š æ˜¾å­˜ä½¿ç”¨å¯¹æ¯”

| é˜¶æ®µ | åŸå§‹æ–¹æ³• | ä¼˜åŒ–æ–¹æ³• | èŠ‚çœ |
|------|---------|---------|------|
| æ¨¡å‹åŠ è½½ | ~10GB (2ä¸ªæ¨¡å‹) | ~5GB (1ä¸ªæ¨¡å‹) | 50% |
| æ¨ç† | ~12GB | ~6GB | 50% |
| å¯è§†åŒ– | ~12GB | ~0.1GB | 99% |

## ğŸ”‘ å…³é”®æŠ€æœ¯ç‚¹

### 1. æ˜¾å¼åˆ é™¤æ¨¡å‹
```python
del model  # åˆ é™¤ Python å¼•ç”¨
```

### 2. æ¸…ç©º CUDA ç¼“å­˜
```python
torch.cuda.empty_cache()  # é‡Šæ”¾ GPU æ˜¾å­˜
```

### 3. æ•°æ®ç»“æ„è®¾è®¡
```python
# ä½¿ç”¨å­—å…¸åˆ—è¡¨å­˜å‚¨ä¸­é—´ç»“æœ
image_data = [
    {
        'img_path': '...',
        'finetuned_depth': None,  # é˜¶æ®µ 1 å¡«å……
        'baseline_depth': None,   # é˜¶æ®µ 2 å¡«å……
        'rgb_image': None,        # é˜¶æ®µ 3 å¡«å……
        'gt_depth': None,         # é˜¶æ®µ 3 å¡«å……
    },
    ...
]
```

## ğŸ“ˆ æ€§èƒ½å½±å“

### æ—¶é—´å¼€é”€
- **åŸå§‹æ–¹æ³•**: å•æ¬¡éå†æ‰€æœ‰å›¾åƒ
- **ä¼˜åŒ–æ–¹æ³•**: ä¸‰æ¬¡éå†æ‰€æœ‰å›¾åƒ
- **æ—¶é—´å¢åŠ **: ~2xï¼ˆä½†é¿å…äº† OOMï¼‰

### æƒè¡¡
- âœ… æ˜¾å­˜éœ€æ±‚å‡åŠ
- âœ… å¯åœ¨æ›´å¤š GPU ä¸Šè¿è¡Œ
- âœ… é¿å… OOM é”™è¯¯
- âš ï¸ è¿è¡Œæ—¶é—´å¢åŠ ï¼ˆä½†ä»å¯æ¥å—ï¼‰

## ğŸ“ æœ€ä½³å®è·µ

### 1. æ‰¹é‡æ¨ç†æ—¶çš„æ˜¾å­˜ç®¡ç†
```python
# æ¨ç†å®Œæˆåç«‹å³é‡Šæ”¾
with torch.no_grad():
    output = model(input)
    result = output.cpu().numpy()  # è½¬åˆ° CPU
del output  # é‡Šæ”¾ GPU tensor
```

### 2. æ¨¡å‹åˆ‡æ¢
```python
# åˆ‡æ¢æ¨¡å‹å‰æ¸…ç†æ˜¾å­˜
del old_model
torch.cuda.empty_cache()
new_model = load_model(...)
```

### 3. ç›‘æ§æ˜¾å­˜ä½¿ç”¨
```python
import torch

# æŸ¥çœ‹å½“å‰æ˜¾å­˜ä½¿ç”¨
allocated = torch.cuda.memory_allocated() / 1024**3  # GB
reserved = torch.cuda.memory_reserved() / 1024**3    # GB
print(f"Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB")
```

## ğŸš€ ä½¿ç”¨å»ºè®®

### å¯¹äºå°æ•°æ®é›†ï¼ˆ< 10 å¼ å›¾åƒï¼‰
- ä½¿ç”¨ä¼˜åŒ–åçš„åˆ†é˜¶æ®µæ–¹æ³•
- è¿è¡Œæ—¶é—´å¢åŠ å¯å¿½ç•¥

### å¯¹äºå¤§æ•°æ®é›†ï¼ˆ> 100 å¼ å›¾åƒï¼‰
- è€ƒè™‘æ‰¹å¤„ç†ä¼˜åŒ–
- å¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–ä¸ºï¼š
  1. æ‰¹é‡é¢„æµ‹ fine-tunedï¼ˆå¦‚ 10 å¼ ä¸€æ‰¹ï¼‰
  2. æ‰¹é‡é¢„æµ‹ baseline
  3. æ‰¹é‡ç”Ÿæˆå¯è§†åŒ–

### å¯¹äºæ˜¾å­˜å……è¶³çš„æƒ…å†µï¼ˆ> 24GBï¼‰
- å¯ä»¥æ¢å¤åŸå§‹çš„åŒæ—¶åŠ è½½æ–¹æ³•
- æ·»åŠ  `--parallel` å‚æ•°æ§åˆ¶

## ğŸ“ ä»£ç ä¿®æ”¹æ€»ç»“

### ä¿®æ”¹çš„æ–‡ä»¶
- `compare_models.py`: é‡æ„ main å‡½æ•°ï¼Œå®ç°åˆ†é˜¶æ®µå¤„ç†

### æ–°å¢çš„æ–‡ä»¶
- `run_comparison.sh`: ä¾¿æ·è¿è¡Œè„šæœ¬
- `MEMORY_OPTIMIZATION_SUMMARY.md`: æœ¬æ–‡æ¡£

### å…³é”®æ”¹åŠ¨
1. å°†å•æ¬¡éå†æ”¹ä¸ºä¸‰é˜¶æ®µéå†
2. åœ¨æ¯ä¸ªé˜¶æ®µåæ˜¾å¼é‡Šæ”¾æ¨¡å‹å’Œæ˜¾å­˜
3. ä½¿ç”¨æ•°æ®ç»“æ„å­˜å‚¨ä¸­é—´ç»“æœ
4. æ·»åŠ è¿›åº¦æç¤ºå’Œé˜¶æ®µè¯´æ˜

## âœ… éªŒè¯æ–¹æ³•

### 1. æ£€æŸ¥æ˜¾å­˜ä½¿ç”¨
```bash
# è¿è¡Œå‰
nvidia-smi

# è¿è¡Œä¸­ï¼ˆå¦ä¸€ä¸ªç»ˆç«¯ï¼‰
watch -n 1 nvidia-smi

# è§‚å¯Ÿæ˜¾å­˜å³°å€¼æ˜¯å¦ < 8GB
```

### 2. æ£€æŸ¥ç»“æœæ­£ç¡®æ€§
```bash
# è¿è¡Œå¯¹æ¯”
./run_comparison.sh

# æ£€æŸ¥è¾“å‡º
ls -lh model_comparison/
```

### 3. éªŒè¯æŒ‡æ ‡
- å¯¹æ¯”å›¾åº”è¯¥æ­£ç¡®ç”Ÿæˆ
- MAE/RMSE æŒ‡æ ‡åº”è¯¥åˆç†
- Fine-tuned åº”è¯¥ä¼˜äº baseline

---

**ä¼˜åŒ–æ—¥æœŸ**: 2025-10-19  
**ä¼˜åŒ–ç›®æ ‡**: å‡å°‘æ˜¾å­˜ä½¿ç”¨ï¼Œé¿å… OOM  
**ä¼˜åŒ–æ•ˆæœ**: æ˜¾å­˜éœ€æ±‚å‡åŠï¼Œå¯åœ¨ 8GB GPU ä¸Šè¿è¡Œ
