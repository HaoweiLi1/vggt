#!/usr/bin/env python3
"""
ä½¿ç”¨ training ä¸­çœŸå®çš„ dataloader æµ‹è¯• AerialMegaDepth æ•°æ®åŠ è½½
è¿™ä¸ªè„šæœ¬ç›´æ¥ä½¿ç”¨ training/config/default.yaml ä¸­çš„é…ç½®æ¥åˆå§‹åŒ– dataloader
"""

import os
import sys
from pathlib import Path
import numpy as np
import matplotlib.pyplot as plt
import logging
import torch
import torch.distributed as dist

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["OPENCV_IO_ENABLE_OPENEXR"] = "1"

# æ·»åŠ è®­ç»ƒç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "training"))

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')

def init_distributed_mode():
    """åˆå§‹åŒ–åˆ†å¸ƒå¼æ¨¡å¼ï¼ˆå•GPUæµ‹è¯•ï¼‰"""
    if not dist.is_initialized():
        # è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ['MASTER_ADDR'] = 'localhost'
        os.environ['MASTER_PORT'] = '12355'
        os.environ['RANK'] = '0'
        os.environ['WORLD_SIZE'] = '1'
        
        # åˆå§‹åŒ–è¿›ç¨‹ç»„
        dist.init_process_group(
            backend='gloo',  # ä½¿ç”¨ gloo åç«¯ï¼ˆCPUï¼‰
            init_method='env://',
            world_size=1,
            rank=0
        )
        print("âœ… åˆ†å¸ƒå¼è¿›ç¨‹ç»„å·²åˆå§‹åŒ–ï¼ˆå•GPUæ¨¡å¼ï¼‰")

def test_with_hydra_config():
    """ä½¿ç”¨ Hydra é…ç½®æµ‹è¯•"""
    print("\n" + "="*70)
    print("æµ‹è¯• 1: ä½¿ç”¨ Hydra é…ç½®åŠ è½½ dataloader")
    print("="*70)
    
    try:
        from hydra import initialize, compose
        from hydra.utils import instantiate
        
        # åˆå§‹åŒ– Hydra
        with initialize(version_base=None, config_path="training/config"):
            # åŠ è½½é…ç½®
            cfg = compose(config_name="default")
            
            print(f"âœ… é…ç½®åŠ è½½æˆåŠŸ")
            print(f"   - å®éªŒåç§°: {cfg.exp_name}")
            print(f"   - å›¾åƒå°ºå¯¸: {cfg.img_size}")
            print(f"   - æ•°æ®é›†é…ç½®:")
            print(f"     ROOT: {cfg.data.train.dataset.dataset_configs[0].ROOT}")
            print(f"     split_file: {cfg.data.train.dataset.dataset_configs[0].split_file}")
            print(f"     segmentation_root: {cfg.data.train.dataset.dataset_configs[0].segmentation_root}")
            print(f"     remove_sky: {cfg.data.train.dataset.dataset_configs[0].get('remove_sky', True)}")
            
            # å®ä¾‹åŒ– dataloader
            print(f"\n   æ­£åœ¨å®ä¾‹åŒ– train dataloader...")
            train_dataloader = instantiate(cfg.data.train, _recursive_=False)
            
            print(f"âœ… Train dataloader åˆ›å»ºæˆåŠŸ")
            print(f"   - æ•°æ®é›†ç±»å‹: {type(train_dataloader.dataset)}")
            print(f"   - æ•°æ®é›†é•¿åº¦: {len(train_dataloader.dataset)}")
            
            # å®ä¾‹åŒ– val dataloader
            print(f"\n   æ­£åœ¨å®ä¾‹åŒ– val dataloader...")
            val_dataloader = instantiate(cfg.data.val, _recursive_=False)
            
            print(f"âœ… Val dataloader åˆ›å»ºæˆåŠŸ")
            print(f"   - æ•°æ®é›†ç±»å‹: {type(val_dataloader.dataset)}")
            print(f"   - æ•°æ®é›†é•¿åº¦: {len(val_dataloader.dataset)}")
            
            return train_dataloader, val_dataloader, cfg, True
            
    except Exception as e:
        print(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None, False


def test_dataloader_iteration(dataloader, split_name="train", num_batches=3):
    """æµ‹è¯• dataloader è¿­ä»£"""
    print("\n" + "="*70)
    print(f"æµ‹è¯• 2: {split_name} dataloader è¿­ä»£æµ‹è¯•")
    print("="*70)
    
    try:
        # è·å– PyTorch DataLoader
        loader = dataloader.get_loader(epoch=0)
        
        print(f"âœ… DataLoader åˆ›å»ºæˆåŠŸ")
        print(f"   - Batch sampler: {type(loader.batch_sampler)}")
        print(f"   - Num workers: {loader.num_workers}")
        
        # è¿­ä»£å‡ ä¸ªæ‰¹æ¬¡
        success_count = 0
        for i, batch in enumerate(loader):
            if i >= num_batches:
                break
                
            try:
                print(f"\n   æ‰¹æ¬¡ {i}:")
                
                # Training dataloader è¿”å›çš„æ˜¯åˆ—è¡¨æ ¼å¼
                if isinstance(batch, list):
                    print(f"     - æ‰¹æ¬¡å¤§å°: {len(batch)} ä¸ªæ ·æœ¬")
                    
                    for sample_idx, sample in enumerate(batch):
                        print(f"     æ ·æœ¬ {sample_idx}:")
                        print(f"       - seq_name: {sample['seq_name']}")
                        print(f"       - frame_num: {sample['frame_num']}")
                        print(f"       - images å½¢çŠ¶: {[img.shape for img in sample['images']]}")
                        print(f"       - depths å½¢çŠ¶: {[d.shape for d in sample['depths']]}")
                        
                        # æ£€æŸ¥æ·±åº¦å€¼
                        for j, depth in enumerate(sample['depths']):
                            valid_depth = depth[depth > 0]
                            if len(valid_depth) > 0:
                                print(f"       - å›¾åƒ {j} æœ‰æ•ˆæ·±åº¦: {len(valid_depth)} åƒç´ , "
                                      f"èŒƒå›´: [{valid_depth.min():.2f}, {valid_depth.max():.2f}]")
                            else:
                                print(f"       - å›¾åƒ {j}: âš ï¸ æ²¡æœ‰æœ‰æ•ˆæ·±åº¦")
                else:
                    # å•ä¸ªæ ·æœ¬æ ¼å¼
                    print(f"     - seq_name: {batch['seq_name']}")
                    print(f"     - frame_num: {batch['frame_num']}")
                    print(f"     - images å½¢çŠ¶: {[img.shape for img in batch['images']]}")
                
                success_count += 1
                
            except Exception as e:
                print(f"     âŒ æ‰¹æ¬¡ {i} å¤„ç†å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
        
        print(f"\n   æ€»ç»“: {success_count}/{num_batches} æ‰¹æ¬¡æˆåŠŸ")
        return success_count == num_batches
        
    except Exception as e:
        print(f"âŒ DataLoader è¿­ä»£å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_segmentation_in_batch(dataloader):
    """æµ‹è¯•æ‰¹æ¬¡ä¸­çš„åˆ†å‰²æ©ç æ•ˆæœ"""
    print("\n" + "="*70)
    print("æµ‹è¯• 3: éªŒè¯åˆ†å‰²æ©ç åœ¨æ‰¹æ¬¡ä¸­çš„åº”ç”¨")
    print("="*70)
    
    try:
        loader = dataloader.get_loader(epoch=0)
        
        # è·å–ä¸€ä¸ªæ‰¹æ¬¡
        batch = next(iter(loader))
        
        print(f"âœ… è·å–æ‰¹æ¬¡æˆåŠŸ")
        
        # Training dataloader è¿”å›åˆ—è¡¨æ ¼å¼
        if isinstance(batch, list):
            print(f"   - æ‰¹æ¬¡å¤§å°: {len(batch)} ä¸ªæ ·æœ¬")
            sample = batch[0]  # å–ç¬¬ä¸€ä¸ªæ ·æœ¬åˆ†æ
        else:
            sample = batch
        
        print(f"   - åºåˆ—: {sample['seq_name']}")
        print(f"   - å›¾åƒæ•°: {sample['frame_num']}")
        
        # åˆ†ææ·±åº¦å›¾
        for i, depth in enumerate(sample['depths']):
            depth_np = depth.cpu().numpy() if hasattr(depth, 'cpu') else depth
            
            total_pixels = depth_np.size
            valid_pixels = (depth_np > 0).sum()
            zero_pixels = (depth_np == 0).sum()
            
            print(f"\n   å›¾åƒ {i}:")
            print(f"     - æ€»åƒç´ : {total_pixels}")
            print(f"     - æœ‰æ•ˆæ·±åº¦: {valid_pixels} ({valid_pixels/total_pixels*100:.1f}%)")
            print(f"     - é›¶å€¼åƒç´ : {zero_pixels} ({zero_pixels/total_pixels*100:.1f}%)")
            
            if valid_pixels > 0:
                valid_depth = depth_np[depth_np > 0]
                print(f"     - æ·±åº¦èŒƒå›´: [{valid_depth.min():.2f}, {valid_depth.max():.2f}]")
                print(f"     - æ·±åº¦å‡å€¼: {valid_depth.mean():.2f}")
        
        # æ£€æŸ¥é›¶å€¼åƒç´ æ¯”ä¾‹æ˜¯å¦åˆç†ï¼ˆåº”è¯¥åŒ…å«å¤©ç©ºåŒºåŸŸï¼‰
        avg_zero_ratio = np.mean([((d > 0).sum() / d.size) for d in sample['depths']])
        
        if 0.5 < avg_zero_ratio < 0.9:
            print(f"\n   âœ… é›¶å€¼åƒç´ æ¯”ä¾‹åˆç† (å¹³å‡æœ‰æ•ˆåƒç´ : {avg_zero_ratio*100:.1f}%)")
            print(f"      è¿™è¡¨æ˜åˆ†å‰²æ©ç å¯èƒ½å·²æ­£ç¡®åº”ç”¨ï¼ˆå¤©ç©ºåŒºåŸŸè¢«ç§»é™¤ï¼‰")
            return True
        else:
            print(f"\n   âš ï¸ é›¶å€¼åƒç´ æ¯”ä¾‹å¼‚å¸¸ (å¹³å‡æœ‰æ•ˆåƒç´ : {avg_zero_ratio*100:.1f}%)")
            return False
            
    except Exception as e:
        print(f"âŒ åˆ†å‰²æ©ç éªŒè¯å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def visualize_batch(dataloader, save_dir="test_training_visualizations"):
    """å¯è§†åŒ–ä¸€ä¸ªæ‰¹æ¬¡"""
    print("\n" + "="*70)
    print("æµ‹è¯• 4: å¯è§†åŒ–æ‰¹æ¬¡æ•°æ®")
    print("="*70)
    
    try:
        os.makedirs(save_dir, exist_ok=True)
        
        loader = dataloader.get_loader(epoch=0)
        batch = next(iter(loader))
        
        # Training dataloader è¿”å›åˆ—è¡¨æ ¼å¼
        if isinstance(batch, list):
            sample = batch[0]  # å–ç¬¬ä¸€ä¸ªæ ·æœ¬
        else:
            sample = batch
        
        num_images = min(2, sample['frame_num'])
        
        fig, axes = plt.subplots(num_images, 3, figsize=(15, 5*num_images))
        if num_images == 1:
            axes = axes.reshape(1, -1)
        
        fig.suptitle(f'Training Dataloader æ‰¹æ¬¡: {sample["seq_name"]}', fontsize=14)
        
        for i in range(num_images):
            # RGB
            img = sample['images'][i]
            if hasattr(img, 'cpu'):
                img = img.cpu().numpy()
            if img.max() <= 1.0:
                img = (img * 255).astype(np.uint8)
            axes[i, 0].imshow(img)
            axes[i, 0].set_title(f'RGB å›¾åƒ {i}')
            axes[i, 0].axis('off')
            
            # Depth
            depth = sample['depths'][i]
            if hasattr(depth, 'cpu'):
                depth = depth.cpu().numpy()
            depth_vis = depth.copy()
            depth_vis[depth_vis == 0] = np.nan
            im = axes[i, 1].imshow(depth_vis, cmap='turbo')
            axes[i, 1].set_title(f'æ·±åº¦å›¾ {i}\næœ‰æ•ˆ: {(depth > 0).sum()}/{depth.size}')
            axes[i, 1].axis('off')
            plt.colorbar(im, ax=axes[i, 1], fraction=0.046)
            
            # Point mask
            if 'point_masks' in sample:
                mask = sample['point_masks'][i]
                if hasattr(mask, 'cpu'):
                    mask = mask.cpu().numpy()
                axes[i, 2].imshow(mask, cmap='gray')
                axes[i, 2].set_title(f'æœ‰æ•ˆç‚¹æ©ç  {i}')
                axes[i, 2].axis('off')
        
        plt.tight_layout()
        
        save_path = os.path.join(save_dir, f'training_batch_{sample["seq_name"]}.png')
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"âœ… å¯è§†åŒ–å·²ä¿å­˜: {save_path}")
        
        plt.close()
        return True
        
    except Exception as e:
        print(f"âŒ å¯è§†åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def compare_with_manual_config():
    """å¯¹æ¯”æ‰‹åŠ¨é…ç½®å’Œ Hydra é…ç½®"""
    print("\n" + "="*70)
    print("æµ‹è¯• 5: å¯¹æ¯”æ‰‹åŠ¨é…ç½® vs Hydra é…ç½®")
    print("="*70)
    
    try:
        from hydra import initialize, compose
        from hydra.utils import instantiate
        from types import SimpleNamespace
        from data.datasets.megadepth_aerial import MegaDepthAerialDataset
        
        # 1. Hydra é…ç½®
        with initialize(version_base=None, config_path="training/config"):
            cfg = compose(config_name="default")
            hydra_dataloader = instantiate(cfg.data.train, _recursive_=False)
            hydra_dataset = hydra_dataloader.dataset
        
        # 2. æ‰‹åŠ¨é…ç½®ï¼ˆä¹‹å‰çš„æµ‹è¯•è„šæœ¬æ–¹å¼ï¼‰
        common_conf = SimpleNamespace(
            img_size=518,
            patch_size=14,
            debug=False,
            training=True,
            get_nearby=False,
            inside_random=False,
            allow_duplicate_img=False,
            repeat_batch=False,
            rescale=True,
            rescale_aug=True,
            landscape_check=True,
            augs=SimpleNamespace(scales=[1.0])
        )
        
        manual_dataset = MegaDepthAerialDataset(
            common_conf=common_conf,
            split="train",
            ROOT="/home/haowei/Documents/vggt/training/dataset_aerialmd/cropped",
            split_file="train.npz",
            segmentation_root="/home/haowei/Documents/vggt/training/dataset_aerialmd/cropped_seg",
            max_depth=2000.0,
            depth_percentile=98.0,
            use_pairs=True,
            expand_ratio=2,
            remove_sky=True,
        )
        
        print(f"âœ… ä¸¤ç§é…ç½®éƒ½æˆåŠŸåˆ›å»º")
        print(f"\n   Hydra é…ç½®:")
        print(f"     - æ•°æ®é›†ç±»å‹: {type(hydra_dataset)}")
        print(f"     - æ•°æ®é›†é•¿åº¦: {len(hydra_dataset)}")
        print(f"     - ROOT: {hydra_dataset.datasets[0].ROOT if hasattr(hydra_dataset, 'datasets') else 'N/A'}")
        
        print(f"\n   æ‰‹åŠ¨é…ç½®:")
        print(f"     - æ•°æ®é›†ç±»å‹: {type(manual_dataset)}")
        print(f"     - æ•°æ®é›†é•¿åº¦: {len(manual_dataset)}")
        print(f"     - ROOT: {manual_dataset.ROOT}")
        print(f"     - segmentation_root: {manual_dataset.segmentation_root}")
        print(f"     - remove_sky: {manual_dataset.remove_sky}")
        
        # å¯¹æ¯”å…³é”®å‚æ•°
        print(f"\n   å…³é”®å‚æ•°å¯¹æ¯”:")
        
        # è·å– Hydra é…ç½®çš„å®é™…æ•°æ®é›†
        actual_dataset = hydra_dataset.datasets[0] if hasattr(hydra_dataset, 'datasets') else hydra_dataset
        
        params_match = True
        if hasattr(actual_dataset, 'segmentation_root'):
            if actual_dataset.segmentation_root == manual_dataset.segmentation_root:
                print(f"     âœ… segmentation_root ä¸€è‡´")
            else:
                print(f"     âŒ segmentation_root ä¸ä¸€è‡´")
                print(f"        Hydra: {actual_dataset.segmentation_root}")
                print(f"        æ‰‹åŠ¨: {manual_dataset.segmentation_root}")
                params_match = False
        
        if hasattr(actual_dataset, 'remove_sky'):
            if actual_dataset.remove_sky == manual_dataset.remove_sky:
                print(f"     âœ… remove_sky ä¸€è‡´")
            else:
                print(f"     âŒ remove_sky ä¸ä¸€è‡´")
                params_match = False
        
        if hasattr(actual_dataset, 'max_depth'):
            if actual_dataset.max_depth == manual_dataset.max_depth:
                print(f"     âœ… max_depth ä¸€è‡´")
            else:
                print(f"     âŒ max_depth ä¸ä¸€è‡´")
                params_match = False
        
        return params_match
        
    except Exception as e:
        print(f"âŒ å¯¹æ¯”å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("\n" + "="*70)
    print("ä½¿ç”¨ Training çœŸå® Dataloader æµ‹è¯• AerialMegaDepth")
    print("="*70)
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼æ¨¡å¼
    init_distributed_mode()
    
    results = {}
    
    # æµ‹è¯• 1: ä½¿ç”¨ Hydra é…ç½®
    train_dataloader, val_dataloader, cfg, success = test_with_hydra_config()
    results['Hydra é…ç½®åŠ è½½'] = success
    
    if not success:
        print("\nâŒ æ— æ³•åŠ è½½é…ç½®ï¼Œæµ‹è¯•ç»ˆæ­¢")
        return
    
    # æµ‹è¯• 2: Train dataloader è¿­ä»£
    results['Train Dataloader è¿­ä»£'] = test_dataloader_iteration(train_dataloader, "train", num_batches=3)
    
    # æµ‹è¯• 3: Val dataloader è¿­ä»£
    results['Val Dataloader è¿­ä»£'] = test_dataloader_iteration(val_dataloader, "val", num_batches=2)
    
    # æµ‹è¯• 4: åˆ†å‰²æ©ç éªŒè¯
    results['åˆ†å‰²æ©ç éªŒè¯'] = test_segmentation_in_batch(train_dataloader)
    
    # æµ‹è¯• 5: å¯è§†åŒ–
    results['æ‰¹æ¬¡å¯è§†åŒ–'] = visualize_batch(train_dataloader)
    
    # æµ‹è¯• 6: é…ç½®å¯¹æ¯”
    results['é…ç½®å¯¹æ¯”'] = compare_with_manual_config()
    
    # æœ€ç»ˆæ€»ç»“
    print("\n" + "="*70)
    print("æµ‹è¯•æ€»ç»“")
    print("="*70)
    
    for test_name, passed in results.items():
        status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
        print(f"   {test_name}: {status}")
    
    total_tests = len(results)
    passed_tests = sum(results.values())
    
    print(f"\n   æ€»è®¡: {passed_tests}/{total_tests} æµ‹è¯•é€šè¿‡")
    
    if passed_tests == total_tests:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Training dataloader æ­£ç¡®ä½¿ç”¨äº† AerialMegaDepth æ•°æ®é›†ï¼")
    else:
        print(f"\nâš ï¸ æœ‰ {total_tests - passed_tests} ä¸ªæµ‹è¯•å¤±è´¥")
    
    print("="*70 + "\n")


if __name__ == "__main__":
    main()
